# KV Store实验报告
## 系统框架
本次`KV Store`实验我们小组选用B+树作为存储索引的数据结构，B+树在面对大量的硬盘文件时有良好的吞吐性能，还可以支持区间查询。在并发控制与并发安全方面，我们采用了基础的文件锁，即同时只能有一个线程对文件进行操作。

### 项目文件
在`/engine_race`目录中，我们新建了如下文件：
+ `BPlusTree.h, BPlusTree.cc`，B+树代码
+ `logger.h`，WAL文件结构

### 正确性与性能测试
#### 正确性测试
在`/test`目录中，我们新建了如下文件：
+ single_small_io_test.cc, single_middle_io_test.cc, single_big_io_test.cc，在不同量级的IO数据下测试单线程工作的正确性，最高为百万级IO。
+ range_test.cc，测试B+树区间查询的正确性
// TODO: 崩溃一致性测试

#### 性能测试
B+树读写吞吐量如下：
| r/w | isSkew | throughput |
| :-: | :-: | :-: |
| w | 0 | 32k/s |
| r | 0 | 78k/s |
| w | 1 | 35k/s |
| r | 1 | 75k/s |


B+树并发效率如下：
| thread_num | read_ratio | isSkew | throughput |
| :-: | :-: | :-: | :-: |
| 1 | 50% | 0 | 43k/s |
| 1 | 50% | 1 | 46k/s |
| 2 | 50% | 0 | 20k/s |
| 2 | 50% | 1 | 22k/s |
| 3 | 50% | 0 | 23k/s |
| 3 | 50% | 1 | 23k/s |
| 4 | 50% | 0 | 22k/s |
| 4 | 50% | 1 | 23k/s |

我们的`KV Store`项目在性能测试下表现一般，经分析有如下几个原因：
+ 底层与文件的交互采用`fread, fwrite`模式，产生阻塞，造成的读写耗时大，
+ B+树索引存在文件内，没有做内存的缓存，每次需要访问节点需要从磁盘中重新读取，若有更改需要及时写回，IO开销大。
+ 并发控制采用文件锁，只能保证安全不能保证效率。

## 优化与创新
### B+树分裂策略改进
本次`KV Store`实验中需要存储的key是不定长的字符串，且以小长度字符串为主。若以传统的按节点度数进行B+树分裂，在索引存储到文件的过程中，文件结构不好设计。若给每个key分配固定的`512Bytes`大小，会造成节点内部有大量的空间浪费，但低于这个值会使得索引的正确性得不到保证。因此我们小组改进了B+树的分裂策略，给B+树的每个节点分配定长的用于存储key的文件块，当节点写满该块时进行分裂，该策略显著减小了B+树单个节点占用的文件块大小，让B+树的每个节点能尽可能多的存储索引记录，有效降低磁盘io次数。

### B+树写放大
采用改进的分裂策略后，需要为节点中的每个key添加在文件块中的位置信息，若key的长度可以很小，那么这些位置信息也会很多，结果还是导致B+树单个节点过大。因此我们小组对插入的key采取了写放大策略，经参数调试后，每个key值的长度设置为20Byte，这样单个节点的信息约占4KB，并且在bench测试时能获得一个较均衡的读写吞吐量。

### B+树区间查询
我们小组完成了B+树的区间查询，流程如下：
+ 定位`lower key`和`upper key`所在的叶节点
+ 利用叶节点的兄弟指针循环访问上一步中定位的每个叶节点
+ 在每个叶节点中提取属于`[lower, upper)`的键值对

## 未完成的开发
### B+树多线程锁添加
样例代码中采用的文件锁虽然能保证并发安全，但是不具备并发扩展性。为了提升并发效率，我们尝试调研了工程代码中的B+树是如何添加锁的，网上的资料指明，B+树有一个升级版本`B-Link-Tree`，除了叶子结点有兄弟指针，内节点也存储了兄弟指针，在`B-Link-Tree`的基础上为每个节点添加节点锁，就能实现高效的并发控制。我们参考了[此链接](https://zhuanlan.zhihu.com/p/24800198)中的节点锁安全性证明，并尝试自己实现节点锁：

+ 对于全部的读操作，在B+树中进行索引的时候，每遍历到一个节点，就对该节点添加读锁，访问结束后读锁释放；
+ 对于全部的写操作，在B+树中进行索引的时候，每遍历到一个节点，就对该节点添加写锁，访问结束后写锁释放。叶节点插入后可能需要向上添加索引记录，但此时原有的父指针指向的父节点可能已经分裂过，需要通过`B-Link-Tree`的内节点兄弟指针寻找正确的父节点，并给它添加写锁，访问结束后写锁释放，循环直到再也不能向上分裂为止。

但是因为并发控制的调试比较繁琐，最终没能完成。

### WAL



## 思考题
### 1. 如何保证和验证Key Value存储引擎的Crash Consistency？

### 2. 基于SSD和HDD的键值存储系统⽐基于NVM更需要考虑如何高效地完成IO操作。目前KV对外存读写数据的方式有以下几种，他们对KV的整体吞吐、IO利用率和内存使用率有何差异？

+ 系统调用read、write、fsync：程序与硬盘交互，存在用户缓冲区和内核缓冲区间的过度，系统定时执行fsync指令，从内核缓冲区写入物理设备。在使用fsync指令的时候会产生阻塞，影响KV的整体吞吐；fsync单次可以从内核缓冲区写入一定量的数据，但是fsync还需要更新文件的metadata，需要两次的磁盘IO，IO利用率不高；数据需要同时在用户缓冲区、内核缓冲区备份，内存使用率高。

+ 系统调用mmap、msync：直接建立物理设备中文件和内存的通道，即使用共享内存。调用msync才会将共享内存中的数据落盘，IO利用率较高；但是调用msync同样会产生阻塞，影响KV的整体吞吐；数据只需要在共享内存中备份即可，内存使用率较低。

+ 异步IO框架libaio、io_uring和SPDK：异步IO旨在使应用运行和IO执行成为并行关系，利用高性能的存储设备提升IO利用率。libaio是早期的Linux异步IO库，使用了较多的系统调用，以及产生少量的内存占用。io_uring显著减少了系统调用次数，并省去了一些占用的内存。SPDK则采取了尽可能避免程序陷入管态的情形。总的来说，异步IO框架具有IO利用率高，内存使用率低的优秀特性，能很大程度提升KV的整体吞吐量，会是之后重点使用的IO接口。

## 小组分工
闭浩扬：崩溃一致性保证及B+树分裂优化设计

张后斌：并发控制与并发安全方面

陈荣钊：B+树代码实现